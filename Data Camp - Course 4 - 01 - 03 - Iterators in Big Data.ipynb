{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19922832\n"
     ]
    }
   ],
   "source": [
    "# ITERATORS IN BIG DATA\n",
    "\n",
    "#Dealing with larger amount of data requires the use of iterators to chop the data and process by loading/processing and discarding data in chunks\n",
    "\n",
    "# LOADING DATA IN CHUNKS - using pandas library funcion read_csv() that allows data to be processed in chunks\n",
    "    # read_csv() allows loading data in chunks\n",
    "    # chunksize argument allows to specify number of chunks\n",
    "    \n",
    "# iterating over Data\n",
    "\n",
    "#lets say we have a csv with coloumn x of which numbers we need to sum\n",
    "#however the number of rws are to large to hold in memory in one go\n",
    "#Thus we have to process the data in chunks\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "result  = []\n",
    "\n",
    "#<-- This converts the data in csv into chunks of data. Which is an iterable object\n",
    "\n",
    "for chunk in pd.read_csv(\"D:\\Data Camp\\data.csv\", chunksize = 1000):   #<-- iterating with for loop over the chunks iterale creatd by read_csv\n",
    "    #Each chunk in above iterable will be a dataframe\n",
    "    result.append(sum(chunk[\"data\"]))  #<-- selecting the coloumn x of the chunk dataframe under iteration and sums it\n",
    "\n",
    "total = sum(result) #<-- the list result now contains the sum of all chunks as elements. whose total sum will give the entire x coloumn sum of big data as required\n",
    "print(total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'en': 97, 'et': 1, 'und': 2}\n"
     ]
    }
   ],
   "source": [
    "# You will process a large csv file of Twitter data in the same way that you processed 'tweets.csv' in previous course\n",
    "\n",
    "# Initialize an empty dictionary: counts_dict - for storing the results of processing the Twitter data\n",
    "counts_dict = {}\n",
    "\n",
    "# Iterate over the file chunk by chunk\n",
    "for chunk in pd.read_csv('D:\\Data Camp\\Course 3-01-Tweets.csv', chunksize=10):\n",
    "\n",
    "    # In the inner loop, iterate over the column 'lang' in chunk by using a for loop. Use the loop variable entry.\n",
    "    for entry in chunk['lang']:\n",
    "        if entry in counts_dict.keys():\n",
    "            counts_dict[entry] += 1\n",
    "        else:\n",
    "            counts_dict[entry] = 1\n",
    "\n",
    "# Print the populated dictionary\n",
    "print(counts_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'en': 97, 'et': 1, 'und': 2}\n"
     ]
    }
   ],
   "source": [
    "# It's good to know how to process a file in smaller, more manageable chunks, \n",
    "#but it can become very tedious having to write and rewrite the same code for the same task each time.\n",
    "\n",
    "## Let's write a user defined function to do this task.\n",
    "\n",
    "# <-- Define the function count_entries(), which has 3 parameters. \n",
    "#     The first parameter is csv_file for the filename, the second is c_size for the chunk size, \n",
    "#     and the last is colname for the column name.\n",
    "\n",
    "# Define count_entries()\n",
    "def count_entries(csv_file, c_size, colname):\n",
    "    \"\"\"Return a dictionary with counts of\n",
    "    occurrences as value for each key.\"\"\"\n",
    "    \n",
    "    # Initialize an empty dictionary: counts_dict\n",
    "    counts_dict = {}\n",
    " \n",
    "    # <--  Iterate over the file in csv_file file by using a for loop. Use the loop variable chunk and iterate over the\n",
    "    #      call to pd.read_csv(), passing c_size to chunksize\n",
    "    \n",
    "    # Iterate over the file chunk by chunk\n",
    "    for chunk in pd.read_csv(csv_file, chunksize =c_size):\n",
    "\n",
    "        # Iterate over the column in DataFrame\n",
    "        for entry in chunk[colname]:\n",
    "            if entry in counts_dict.keys():\n",
    "                counts_dict[entry] += 1\n",
    "            else:\n",
    "                counts_dict[entry] = 1\n",
    "\n",
    "    # Return counts_dict\n",
    "    return counts_dict\n",
    "\n",
    "# Call count_entries(): result_counts\n",
    "result_counts = count_entries('D:\\Data Camp\\Course 3-01-Tweets.csv', 10, 'lang')\n",
    "\n",
    "# Print result_counts\n",
    "print(result_counts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
